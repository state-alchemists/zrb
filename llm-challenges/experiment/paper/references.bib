@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@article{jimenez2023swe,
  title={SWE-bench: Can language models resolve real-world github issues?},
  author={Jimenez, Carlos E and Yang, John and Wettig, Alexander and Yao, Shunyu and Pei, Kexin and Press, Ofir and Kaplan, Karthik R},
  journal={arXiv preprint arXiv:2310.06770},
  year={2023}
}

@article{austin2021program,
  title={Program synthesis with large language models},
  author={Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and others},
  journal={arXiv preprint arXiv:2108.07732},
  year={2021}
}

@article{openai2024gpt4,
  title={GPT-4 technical report},
  author={{OpenAI}},
  journal={arXiv preprint arXiv:2303.08774},
  year={2024}
}

@article{gemini2024,
  title={Gemini: A family of highly capable multimodal models},
  author={{Google DeepMind}},
  journal={arXiv preprint arXiv:2312.11805},
  year={2024}
}

@article{deepseek2024,
  title={Deepseek-coder: When the large language model meets programming -- the rise of code intelligence},
  author={{DeepSeek-AI}},
  journal={arXiv preprint arXiv:2401.14196},
  year={2024}
}

@article{glm2024,
  title={ChatGLM: A family of large language models from GLM-130B to GLM-4 all tools},
  author={Zeng, Aohan and Liu, Xiao and Du, Zhengxiao and Wang, Zihan and Lai, Hanyu and Ding, Ming and Yang, Zhuoyi and Xu, Yifan and Zheng, Wendi and Xia, Xiao and others},
  journal={arXiv preprint arXiv:2406.12793},
  year={2024}
}

@article{qwen2024,
  title={Qwen2 technical report},
  author={{Qwen Team}},
  journal={arXiv preprint arXiv:2407.10671},
  year={2024}
}

@article{classexplore,
  title={ClassEval: A manually-crafted benchmark for evaluating LLMs on class-level code generation},
  author={Du, Xueying and Liu, Mingwei and Wang, Kaixin and Wang, Hanlin and Liu, Jiayi and Chen, Yixuan and Feng, Jiyang and Sha, Chaosheng and Xia, Xin and Li, Shuai},
  journal={arXiv preprint arXiv:2308.01861},
  year={2023}
}

@article{repobench,
  title={RepoBench: Benchmarking repository-level code auto-completion systems},
  author={Liu, Mingwei and Du, Xueying and Xia, Hanbin and Wang, Kaixin and Liu, Jiayi and Liu, Yuxiang and Li, Liang and Chen, Shuai and Zhang, Yuxian and Liu, Yanzhao and others},
  journal={arXiv preprint arXiv:2306.03091},
  year={2023}
}

@article{hendrycks2021measuring,
  title={Measuring coding challenge competence with APPS},
  author={Hendrycks, Dan and Basart, Steven and Kadavath, Saurav and Mazeika, Mantas and Arora, Akul and Guo, Ethan and Burns, Collin and Puranik, Samir and Steinhardt, Justin and Song, Dawn},
  journal={arXiv preprint arXiv:2105.09938},
  year={2021}
}

@article{fu2023gptscore,
  title={GPTScore: Evaluate as you desire},
  author={Fu, Jinlan and Ng, See-Kiong and Jiang, Zhengbao and Liu, Pengfei},
  journal={arXiv preprint arXiv:2302.04166},
  year={2023}
}

@article{evaluating2024llm,
  title={Evaluating LLM-guided software programming: A study on GPT-3.5, GPT-4, and CodeLlama},
  author={Zhang, Yang and Zhang, Yifan and Chen, Yifan and Liu, Yihong},
  journal={arXiv preprint arXiv:2402.14261},
  year={2024}
}

@article{qinetal2023toolllm,
  title={ToolLLM: Facilitating large language models to master 16000+ real-world APIs},
  author={Qin, Yujia and Liang, Shihao and Ye, Yining and Zhu, Kunlun and Yan, Lan and Lu, Yaxi and Lin, Yankai and Cong, Xin and Tang, Xiangru and Qian, Bill and others},
  journal={arXiv preprint arXiv:2307.16789},
  year={2023}
}

@article{efficiency2024,
  title={Efficiency challenges in large language model inference: A survey},
  author={Wan, Zhongwei and Wang, Xin and Liu, Che and Alam, Samiul and Zheng, Zhongjie and Qu, Shen and Zhang, Yanjun and Zhu, Qinan and Zhang, Zhilin and Chen, Mosharaf and others},
  journal={arXiv preprint arXiv:2412.07016},
  year={2024}
}

@misc{our_dataset,
  title={LLM Challenge Experiment Dataset},
  author={{Anonymous Authors}},
  year={2026},
  howpublished={\url{https://github.com/[anonymous]/llm-challenge-experiment}}
}
